{"cells":[{"metadata":{"_uuid":"20e28ad0a2f8d8948754cf655c7ca78d601270e1"},"cell_type":"markdown","source":"## 0. Configure Package Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\n\n# import nltk\n# nltk.download()  # Download text data sets, including stop words\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## 1. Import the  Dataset"},{"metadata":{"trusted":true,"_uuid":"b89712a005997175c1151b72cb963685df071999"},"cell_type":"code","source":"# \"header=0\" indicates that the first line of the file contains column names, \n# \"delimiter=\\t\" indicates that the fields are separated by tabs, and \n# quoting=3 tells Python to ignore doubled quotes.\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)\ntest = pd.read_csv(\"../input/testData.tsv\", header=0, delimiter='\\t', quoting=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82c969391921affa54a9d3f2259c036ad994cd13"},"cell_type":"markdown","source":"## 2. Preview the Dataset"},{"metadata":{"_uuid":"9715a877ff8910c6c4f9bfb073b89fc1e81bcf12"},"cell_type":"markdown","source":"### Training Data"},{"metadata":{"trusted":true,"_uuid":"d4938da6425eefa3e7d3cd0c6b46c300602b3d13"},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2abbdf305aa9fd4baa592f2ab8b7465682506bb8"},"cell_type":"code","source":"# Display the dimensions of the dataset.\nrows = train.shape[0]\ncolumns = train.shape[1]\nfeature_set = train.columns.values\nprint('Total Number of Features: ', columns)\nprint('Total Number of Instances: ', rows)\nprint('Feature Set includes: ', feature_set)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b4dadd30133de42aee77665e4599ab986d5c832"},"cell_type":"markdown","source":"### Testing Data"},{"metadata":{"trusted":true,"_uuid":"e1f42ba976efa20774af914bce0897de3dd8a092"},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"146b0e7e1311d6d983531d3cd784d4dbd0f870d3"},"cell_type":"code","source":"# Display the dimensions of the dataset.\nrows = test.shape[0]\ncolumns = test.shape[1]\nfeature_set = test.columns.values\nprint('Total Number of Features: ', columns)\nprint('Total Number of Instances: ', rows)\nprint('Feature Set includes: ', feature_set)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53623345befa9633771b6e11e6e1567a25c61077"},"cell_type":"markdown","source":"## 3. Data Cleaning and Text Preprocessing\n- Removing **HTML Markup**: The BeautifulSoup Package\n- Dealing with **Punctuation**, **Numbers** and **Stopwords**: NLTK and regular expressions\n- Converting our reviews to lower case and split them into individual words: tokenization"},{"metadata":{"trusted":true,"_uuid":"db504288d17a48aae30bbd92d6342b5ae8305492"},"cell_type":"code","source":"def review_to_words( raw_review ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    #    \n    # 1. Remove HTML markup.\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. Remove non-letters.     \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. Convert to lower case, split into individual words.\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set.\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. Remove stop words\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words )) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cabe94511eaccbf86d77061c0652bb587b7393f7"},"cell_type":"code","source":"# Get the number of reviews based on the dataframe column size\nnum_reviews = train[\"review\"].size\n\nprint(\"Cleaning and parsing the training set movie reviews....\")\n\n# Initialize an empty list to hold the clean reviews.\nclean_train_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length of the movie review list.\nfor i in range( 0, num_reviews ):\n    # If the index is evenly divisible by 1000, print a message.\n    if( (i+1)%1000 == 0 ):\n        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ) ) \n    # Call our function for each one, and add the result to the list of clean reviews.\n    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c71a7235f1229f808538547779955c85304da3c"},"cell_type":"code","source":"# Get the number of reviews based on the dataframe column size\nnum_reviews = test[\"review\"].size\n\nprint(\"Cleaning and parsing the testing set movie reviews....\")\n\n# Initialize an empty list to hold the clean reviews.\nclean_test_reviews = []\n\n# Loop over each review; create an index i that goes from 0 to the length of the movie review list.\nfor i in range( 0, num_reviews ):\n    # If the index is evenly divisible by 1000, print a message.\n    if( (i+1)%1000 == 0 ):\n        print(\"Review %d of %d\\n\" % ( i+1, num_reviews ) ) \n    # Call our function for each one, and add the result to the list of clean reviews.\n    clean_test_reviews.append( review_to_words( test[\"review\"][i] ))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9f04c65263b0a46ce7bcf8bb1eef261b5a81241"},"cell_type":"markdown","source":"## 4. Feature Processing\n- Bag of Words\n- TF-IDF\n- Word2vec"},{"metadata":{"_uuid":"a058094b4bcf669be15cd3a5420c829248f5821c"},"cell_type":"markdown","source":"### 4.1 Bag of Words + Random Forest"},{"metadata":{"_uuid":"d03c67470aa42230c5a3641b49ca2d40491cb0c6"},"cell_type":"markdown","source":"### 4.1.1 Creating Features from a Bag of Words"},{"metadata":{"trusted":true,"_uuid":"96886233591a03827a307b458fc93c0d249bc629"},"cell_type":"code","source":"print(\"Creating the bag of words....\")\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of strings.\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n\n# Numpy arrays are easy to work with, so convert the result to an array.\ntrain_data_features = train_data_features.toarray()\n\nvocab = vectorizer.get_feature_names()\n\n# Sum up the counts of each vocabulary word.\ndist = np.sum(train_data_features, axis=0)\n\n# For each, print the vocabulary word and the number of times it \n# appears in the training set\nfor tag, count in zip(vocab, dist):\n    print(count, tag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"901cec8c609dd58788ac8bdfd0a62d9ac48a15d3"},"cell_type":"markdown","source":"### 4.1.2 Random Forest based on BoW"},{"metadata":{"trusted":true,"_uuid":"68b6f9e8c537ee02b2b069e7094c707228bfab79"},"cell_type":"code","source":"print(\"Training the random forest...\")\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize a Random Forest classifier with 100 trees\nforest = RandomForestClassifier(n_estimators = 100) \n\n# Fit the forest to the training set, using the bag of words as \n# features and the sentiment labels as the response variable\n#\n# This may take a few minutes to run\nforest = forest.fit( train_data_features, train[\"sentiment\"] )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5300b40ab26c96fcd500d1ea829b821c8e6dd9d"},"cell_type":"markdown","source":"### 4.1.3 Creating a Submission with 4.1.2 model"},{"metadata":{"trusted":true,"_uuid":"cd3949f843f1c5f39e7f8dbe24f8582b6a8c605f"},"cell_type":"code","source":"# Get a bag of words for the test set, and convert to a numpy array\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# Use the random forest to make sentiment label predictions\nresult = forest.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2a2b0fe3f99a52288fb55cb99f38f36e352c66a"},"cell_type":"markdown","source":"### 4.2 TF-IDF + Naive Bayes"},{"metadata":{"_uuid":"4980222c4a769bb33cfe99656f23b27b18fa65b0"},"cell_type":"markdown","source":"### 4.2.1 Creating Features from a TF-IDF"},{"metadata":{"trusted":true,"_uuid":"0fffc7ee051c6cbc2ef8132cf84afe6c0b9c7088","collapsed":true},"cell_type":"code","source":"print(\"Creating the TF-IDF....\")\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize the \"TfidfVectorizer\" object, which is scikit-learn's TF-IDF tool.  \ntfidf = TfidfVectorizer(min_df = 2,\n                        max_features = None,\n                        strip_accents = 'unicode',\n                        analyzer = 'word',\n                        token_pattern = r'\\w{1,}',\n                        ngram_range = (1, 3), \n                        use_idf = 1,\n                        smooth_idf = 1,\n                        sublinear_tf = 1,\n                        stop_words = 'english') \n\n# Merge traning and testing data in order to vectorize.\nclean_all_reviews = clean_train_reviews + clean_test_reviews\ntrain_index = len(clean_train_reviews)\n\n# fit_transform() does two functions: First, it fits the model\n# and learns the vocabulary; second, it transforms our training data\n# into feature vectors. The input to fit_transform should be a list of strings.\nall_data_features = tfidf.fit_transform(clean_all_reviews)\n\n# Recover training and testing data.\ntrain_data_features = all_data_features[:train_index]\ntest_data_features = all_data_features[train_index:]\n\nvocab = tfidf.get_feature_names()\n\n# Sum up the counts of each vocabulary word\ndist = np.sum(train_data_features, axis=0)\n\n# For each, print the vocabulary word and the number of times it \n# appears in the training set\nfor tag, count in zip(vocab, dist):\n    print(count, tag)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46e4d5ee2180728c8684f9eb9080049786234e5e"},"cell_type":"markdown","source":"### 4.2.2 Naive Bayes based on TF-IDF"},{"metadata":{"trusted":true,"_uuid":"6353ddedb5972d12ed7f60821be07cf92bbd0bb7"},"cell_type":"code","source":"print(\"Training the Naive Bayes...\")\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Initialize a Random Forest classifier with 100 trees\nmodel_NB = MultinomialNB() \n\nfrom sklearn.model_selection import cross_val_score\nprint(\"Score of 10-fold CV: \", np.mean(cross_val_score(model_NB, train_data_features, train[\"sentiment\"], cv=10, scoring='roc_auc')))\n\n# Fit the naive bayes to the training set, using the bag of words as \n# features and the sentiment labels as the response variable\n#\n# This may take a few minutes to run\nmodel_NB = model_NB.fit(train_data_features, train[\"sentiment\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a04bf8cb2982c6df226716e294615cf3d07be29a"},"cell_type":"markdown","source":"### 4.2.3 Creating a Submission with 4.2.2 model"},{"metadata":{"trusted":true,"_uuid":"675f8fe47e6fa6a8b230346317ae0b0ecbe2c876"},"cell_type":"code","source":"# Use the random forest to make sentiment label predictions\nresult = model_NB.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"TF_IDF_NAIVE_BAYES_model.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0acd8911cc866172744cc7ff1a99b61430f98e3d"},"cell_type":"markdown","source":"### 4.3 TF-IDF + Logistic Regression"},{"metadata":{"_uuid":"215679c47cbb3c28ea2c9f12eb233ce75b33af34"},"cell_type":"markdown","source":"### 4.3.2 Logistic Regression based on TF-IDF"},{"metadata":{"trusted":true,"_uuid":"eb7f0ec795fa322ab424fbe0fe21029a770ec7a1"},"cell_type":"code","source":"print(\"Training the Logistic Regression...\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# \ngrid_values = {'C': [1, 15, 30, 50]}  \nmodel_LR = GridSearchCV(\n    LogisticRegression(penalty='l2', dual=True, random_state=0), \n    grid_values, \n    scoring='roc_auc', \n    cv=20)\n\nmodel_LR = model_LR.fit(train_data_features, train[\"sentiment\"])\nprint(model_LR.cv_results_, '\\n', model_LR.best_params_, model_LR.best_score_)\n\n# Fit the logistic regression to the training set, using the bag of words as \n# features and the sentiment labels as the response variable\n#\n# This may take a few minutes to run\n# model_LR = LogisticRegression(penalty='l2', dual=True, random_state=0)\n# model_LR = model_LR.fit(train_data_features, train[\"sentiment\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9bc50231ab4973f0c8122f288d21e8db030d2a3"},"cell_type":"markdown","source":"### 4.3.3 Creating a Submission with 4.3.2 model"},{"metadata":{"trusted":true,"_uuid":"2238e18032d573d9b96552d70c456baa5a88d56e"},"cell_type":"code","source":"# Use the logistic regression to make sentiment label predictions\nresult = model_LR.predict(test_data_features)\n\n# Copy the results to a pandas dataframe with an \"id\" column and\n# a \"sentiment\" column\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\n# Use pandas to write the comma-separated output file\noutput.to_csv( \"TF_IDF_LR_model.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"074c6f99ff1433dcece207489996f51b189d369d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}