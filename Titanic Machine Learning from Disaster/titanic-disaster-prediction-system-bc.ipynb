{"cells":[{"metadata":{"_uuid":"735af9b70d7bb82a150b8b83033ba9a7d0f9dcac"},"cell_type":"markdown","source":"# 0. Configure Package Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# 1. Import the Dataset"},{"metadata":{"trusted":true,"_uuid":"53c2cdebed17e0e0625e9bdce325eb47c5c9925c"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e8623a336ec352eb1bb75007d002afbae0326a0"},"cell_type":"markdown","source":"# 2. Preview the Dataset"},{"metadata":{"_uuid":"94fd0fedb98f2ec2063efaa133a8cda6f6d85a9f"},"cell_type":"markdown","source":"## Training Data"},{"metadata":{"trusted":true,"_uuid":"2deb6c48ab174bc86097bd338353b5bfc745fcf0"},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"422baaf9304f4032e2a2cb2fb336b08b7ee0fd7d"},"cell_type":"code","source":"# Display the dimensions of the dataset.\nrows = train.shape[0]\ncolumns = train.shape[1]\nfeature_set = train.columns.values\nprint('Total Number of Features: ', columns)\nprint('Total Number of Instances: ', rows)\nprint('Feature Set includes: ', feature_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e898efc76f7e6a76306574d2a4ab108eeb17a719"},"cell_type":"code","source":"# Return column names, non-null numbers.\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4e2ab797503ca22fc90a134d77eed49e7800c01"},"cell_type":"code","source":"# Return the statistics for all numeric variables.\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22fe1b24a232603643905c2ec1ebb9fb47f99db2"},"cell_type":"markdown","source":"## Testing Data"},{"metadata":{"trusted":true,"_uuid":"e9fe18954706edd5c96ab65cc393b1b3d44a6317"},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f679224d680296597295c838c570743bb2e494a"},"cell_type":"code","source":"# Display the dimensions of the dataset.\nrows = test.shape[0]\ncolumns = test.shape[1]\nfeature_set = test.columns.values\nprint('Total Number of Features: ', columns)\nprint('Total Number of Instances: ', rows)\nprint('Feature Set includes: ', feature_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fda9e03b14c57f345145e7dd541b36402be769a4"},"cell_type":"code","source":"# Return column names, non-null numbers.\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"983154d6579c685a657bee982d3466cf33882b34"},"cell_type":"code","source":"# Return the statistics for all numeric variables.\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c403a1793e38a4c2054864346784dd3ed31840e1"},"cell_type":"markdown","source":"# 3. Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"eb10ef872239092bfc411ddbb1ecb90f0e8ea3c9"},"cell_type":"markdown","source":"## 3.1 Feature Analysis"},{"metadata":{"_uuid":"774ac5b9db175b51da5079e465435696ac7fe5bf"},"cell_type":"markdown","source":"### 3.1.1 Univariate Analysis"},{"metadata":{"trusted":true,"_uuid":"9738697e5d4d787e3d68e536594e4a864d8cf11f"},"cell_type":"code","source":"# Target Feature Analysis.\ntrain['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc547f7fde91ff1ff331f439e59e6b15853396b"},"cell_type":"code","source":"# Compute pairwise correlation of numeric variables, excluding ID, NA/null values.\ntrain_corr = train.drop('PassengerId',axis=1).corr()\ntrain_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68fcee5502d76f4514b79c8fd5af364b1cbdcc0a"},"cell_type":"code","source":"# Display heatmap of pairwise correlation.\na = plt.subplots(figsize=(15,9)) # Resize canvas \na = sns.heatmap(train_corr, vmin=-1, vmax=1 , annot=True , square=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c758de5576b919aae4118fd8810cc0103fdffb97"},"cell_type":"markdown","source":"### 3.1.2 Bivariate Analysis"},{"metadata":{"trusted":true,"_uuid":"6f84d7e1622100fb3da8f1e71dfbc1b3b78dc686"},"cell_type":"code","source":"# Association between \"Pclass\" and \"Survived\".\ntrain[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2219774b5b7c024130cadc7a8c4241b0bfd6d4e6"},"cell_type":"code","source":"# Association between \"Sex\" and \"Survived\".\ntrain[['Sex','Survived']].groupby(['Sex']).mean().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"886b7ce92f3201fe0a345a5e9e6283d58b99e3e1"},"cell_type":"code","source":"# Association between \"Parch\" and \"Survived\".\ntrain[['Parch','Survived']].groupby(['Parch']).mean().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b352fe9bab8b334040536ddaefaf77377b93b6b"},"cell_type":"code","source":"# Association between \"Age\" and \"Survived\".\ng = sns.FacetGrid(train, col='Survived',size=5)\ng.map(plt.hist, 'Age', bins=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddfa9632e0d36e8778bc90e6600ca73c4e6afee9"},"cell_type":"code","source":"# Association between \"Embarked\" and \"Survived\".\nsns.countplot('Embarked',hue='Survived',data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95598df794f411021254dbf5605556aad29e43a7"},"cell_type":"markdown","source":"## 3.2. Feature Processing"},{"metadata":{"trusted":true,"_uuid":"54e878e17c3c82a06473e5a6040962668cf50cf1"},"cell_type":"code","source":"# Merge training and testing data.\ntest['Survived'] = 0\ntrain_test = pd.concat((train, test)).reset_index(drop=True)\ntrain_test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"299102d0ffac4b720efce327fed1809a9a006208"},"cell_type":"markdown","source":"### Categorical Variables"},{"metadata":{"trusted":true,"_uuid":"b0b0a27be983c02f2d886ff3a645511f773add6e"},"cell_type":"code","source":"# One-Hot-Encoder for categorical variable.\ntrain_test['SibSp_Parch'] = train_test['SibSp'] + train_test['Parch']  # Merge siblings & spouses and parents & children\ntrain_test['Embarked'].fillna(train_test['Embarked'].mode()[0],inplace=True) # Fill null with mode value\ntrain_test = pd.get_dummies(train_test,columns = ['Pclass','Sex','SibSp','Parch','SibSp_Parch','Embarked']) \ntrain_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03e46e2a516cafa00e565c03cd656e9c32ef992f"},"cell_type":"code","source":"# Drop other useless categorical features.\ntrain_test = train_test.drop(['PassengerId','Name','Cabin','Ticket'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80d0eaefaa12b4e60c9785c8f8be651ef93de8dc"},"cell_type":"markdown","source":"### Numerical Variables"},{"metadata":{"trusted":true,"_uuid":"4efb6fd0c442ec39181a6e1dac6779bfe6135c7b"},"cell_type":"code","source":"# Fill null with mean value.\ntrain_test['Fare'].fillna(train_test['Fare'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93037372be9abfc5e125970a0e088fb4e0982b61"},"cell_type":"code","source":"train_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60a388422a3ecde6b4dad7e71dbadeb08df63dd0"},"cell_type":"code","source":"# Populate \"Age\" with a regression model.\n# Drop original target feature and create training and testing data.\nmissing_age = train_test.drop(['Survived'],axis=1)            \n\nmissing_age_train = missing_age[missing_age['Age'].notnull()] # Create training data\nmissing_age_test = missing_age[missing_age['Age'].isnull()]   # Create testing data\n\nmissing_age_train_X = missing_age_train.drop(['Age'], axis=1) # Create X\nmissing_age_train_Y = missing_age_train['Age']                # Create Y\nmissing_age_test_X = missing_age_test.drop(['Age'], axis=1)\n\n# Data standardization and train and standardize with test sets.\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nss.fit(missing_age_train_X)\nmissing_age_X_train = ss.transform(missing_age_train_X)\nmissing_age_X_test = ss.transform(missing_age_test_X)\n\n# Predict age (null) with Bayes model.\nfrom sklearn import linear_model\nlin = linear_model.BayesianRidge()\n\nlin.fit(missing_age_train_X,missing_age_train_Y)\n\n# Populate the data set with predicted values.\ntrain_test.loc[(train_test['Age'].isnull()), 'Age'] = lin.predict(missing_age_test_X)\n\n# Bin continous values into discrete intervals (0-10, 10-18, 18-30, 30-50, 50-100).\ntrain_test['Age'] = pd.cut(train_test['Age'], bins=[0,10,18,30,50,100],labels=[1,2,3,4,5])\n\n# One-hot-encoder for \"Age\".\ntrain_test = pd.get_dummies(train_test,columns=['Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"458229f7c1d6e298f95c9983aaa70d606627c251"},"cell_type":"code","source":"# Recover dataset into training and testing data.\ntrain_data = train_test[:891]\ntest_data = train_test[891:]\n# Create X and Y\ntrain_data_X = train_data.drop(['Survived'],axis=1)\ntrain_data_Y = train_data['Survived']\ntest_data_X = test_data.drop(['Survived'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e256ed318e3646609ed8555351de1d962873364e"},"cell_type":"markdown","source":"Linear models require standardized data to be modeled, while tree models do not require standardized data."},{"metadata":{"trusted":true,"_uuid":"d4237984e664c2f49ff08db9596aa0b9efb7c1ef"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss2 = StandardScaler()\nss2.fit(train_data_X)\ntrain_data_X_sd = ss2.transform(train_data_X)\ntest_data_X_sd = ss2.transform(test_data_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5dac19ab06a0b8f2501e52a6f5a516c2ea6ff45"},"cell_type":"markdown","source":"# 4. Modeling"},{"metadata":{"_uuid":"61dc9e6cad5b872d0e5b7436cc2e4c9523a2da7b"},"cell_type":"markdown","source":"## 4.1 Build Models\n- Single model: \n  - Random Forest\n  - Logistic Regression\n  - SVM\n  - XGBOOST\n  - GDBT\n- Multiple model combination: \n  - ensemble\n  - voting\n  - stacking"},{"metadata":{"trusted":true,"_uuid":"0e5e4334f64b02e40af0a17abbdb0176b755ca5a"},"cell_type":"code","source":"# ======Random Forest======\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF_model = RandomForestClassifier(n_estimators=150,min_samples_leaf=3,max_depth=6,oob_score=True)\nRF_model.fit(train_data_X,train_data_Y)\n\n# Random forests are modeled with randomly selected features, so the results may be slightly different each time.\n# If the score is good enough, you can save the model and use it next time.\n# from sklearn.externals import joblib\n# joblib.dump(RF_model, 'rf10.pkl')\n\n# Predict and Output \"submission.csv\" file.\nresult = RF_model.predict(test_data_X)\noutput = pd.DataFrame(data={\"PassengerId\":test[\"PassengerId\"], \"Survived\":result})\noutput.to_csv(\"RF_model.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"726474e202481a15738bc9e5d0a44c6c081c4a7d"},"cell_type":"code","source":"# ======Logistic Regression======\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Build model and optimise hyperparameters with GridSearchCV.\nLR_model = LogisticRegression()\nparam = {'C':[0.001,0.01,0.1,1,10], \"max_iter\":[100,250]}\nclf = GridSearchCV(LR_model,param,cv=5,n_jobs=-1,verbose=1,scoring=\"roc_auc\")\nclf.fit(train_data_X_sd, train_data_Y)\n\n# Output optimal param.\nprint(clf.best_params_)\n\n# Pass the best parameters into the training model.\n# LR_model = LogisticRegression(clf.best_params_)\nLR_model = LogisticRegression(C=1,max_iter=100)\nLR_model.fit(train_data_X_sd, train_data_Y)\n\n# Predict and Output \"submission.csv\" file.\nresult = LR_model.predict(test_data_X_sd)\noutput = pd.DataFrame(data={\"PassengerId\":test[\"PassengerId\"], \"Survived\":result})\noutput.to_csv(\"LR_model.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23c79cc6a7ddce55b89fbd37eb400405c5f1e471"},"cell_type":"code","source":"# ======SVM======\nfrom sklearn import svm\n\n# Build model and optimise hyperparameters with GridSearchCV.\nsvc = svm.SVC()\nclf = GridSearchCV(svc,param,cv=5,n_jobs=-1,verbose=1,scoring=\"roc_auc\")\nclf.fit(train_data_X_sd,train_data_Y)\n\n# Output optimal param.\nprint(clf.best_params_)\n\n# Pass the best parameters into the training model.\nsvc = svm.SVC(C=1,max_iter=250)\nsvc.fit(train_data_X_sd,train_data_Y)\n\n# Predict and Output \"submission.csv\" file.\nresult = svc.predict(test_data_X_sd)\noutput = pd.DataFrame(data={\"PassengerId\":test[\"PassengerId\"], \"Survived\":result})\noutput.to_csv(\"SVM_model.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d845167622b38ef70860dc1135655f9f1d889f56"},"cell_type":"code","source":"# ======Gradient Boosting======\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Build model.\ngbdt = GradientBoostingClassifier(learning_rate=0.7,max_depth=6,n_estimators=100,min_samples_leaf=2)\ngbdt.fit(train_data_X,train_data_Y)\n\n# Predict and Output \"submission.csv\" file.\nresult = gbdt.predict(test_data_X)\noutput = pd.DataFrame(data={\"PassengerId\":test[\"PassengerId\"], \"Survived\":result})\noutput.to_csv(\"GDBT_model.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee2ea84fe6b9e21b1854bd183a2188d2db396ff6"},"cell_type":"code","source":"# ======XGBOOST======\nimport xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(n_estimators=150,min_samples_leaf=3,max_depth=6)\nxgb_model.fit(train_data_X,train_data_Y)\n\n# Predict and Output \"submission.csv\" file.\nresult = xgb_model.predict(test_data_X)\noutput = pd.DataFrame(data={\"PassengerId\":test[\"PassengerId\"], \"Survived\":result})\noutput.to_csv(\"XGB_model.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceadab46289c63ac6ea09456cf4309423490f77f"},"cell_type":"code","source":"# ======voting======\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Build four models: LR, XGBoost, RF, GDBT.\nlr = LogisticRegression(C=0.1,max_iter=100)\n\nxgb_model = xgb.XGBClassifier(max_depth=6,min_samples_leaf=2,n_estimators=100,num_round = 5)\n\nrf = RandomForestClassifier(n_estimators=200,min_samples_leaf=2,max_depth=6,oob_score=True)\n\ngbdt = GradientBoostingClassifier(learning_rate=0.1,min_samples_leaf=2,max_depth=6,n_estimators=100)\n\n# Uses predicted class labels for majority rule voting.\nvot = VotingClassifier(estimators=[('lr', lr), ('rf', rf),('gbdt',gbdt),('xgb',xgb_model)], voting='hard')\nvot.fit(train_data_X_sd,train_data_Y)\n\n# Predict and Output \"submission.csv\" file.\nresult = vot.predict(test_data_X_sd)\noutput = pd.DataFrame(data={\"PassengerId\":test[\"PassengerId\"], \"Survived\":result})\noutput.to_csv(\"VOT_Ensemble.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47ad88b5caeb7ea9a452d977b7c0915c21136dd7"},"cell_type":"code","source":"# # ======stacking======\n# # Divide dataset: training features, testing features, target feature.\n# X = train_data_X_sd\n# X_predict = test_data_X_sd\n# y = train_data_Y\n\n# from sklearn.linear_model import LogisticRegression\n# from sklearn import svm\n# import xgboost as xgb\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import GradientBoostingClassifier\n\n# clfs = [LogisticRegression(C=0.1,max_iter=100),\n#         xgb.XGBClassifier(max_depth=6,n_estimators=100,num_round = 5),\n#         RandomForestClassifier(n_estimators=100,max_depth=6,oob_score=True),\n#         GradientBoostingClassifier(learning_rate=0.3,max_depth=6,n_estimators=100)]\n\n# # Create  n_folds = 5.\n# from sklearn.model_selection import StratifiedKFold\n# n_folds = 5\n# # skf = list(StratifiedKFold(y, n_folds))\n# skf = StratifiedKFold(y, n_folds)\n\n# # Create a zero matrix.\n# dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n# dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n\n# # Build model.\n# for j, clf in enumerate(clfs):\n#     '''Train each single model in turn'''\n#     dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))\n#     for i, (train, test) in enumerate(skf):\n#         '''The i part is used as the prediction, \n#         the rest part is used to train the model, \n#         and the predicted output is obtained as the new feature of the i part'''\n#         X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n#         clf.fit(X_train, y_train)\n#         y_submission = clf.predict_proba(X_test)[:, 1]\n#         dataset_blend_train[test, j] = y_submission\n#         dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n#     '''For the test set, the predicted mean value of the k models is directly used as the new feature'''\n#     dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n\n# # Use to build the second layer model.\n# clf2 = LogisticRegression(C=0.1,max_iter=100)\n# clf2.fit(dataset_blend_train, y)\n# y_submission = clf2.predict_proba(dataset_blend_test)[:, 1]\n\n# test = pd.read_csv(\"test.csv\")\n\n# # Predict and Output \"submission.csv\" file.\n# result = clf2.predict(dataset_blend_test)\n# output = pd.DataFrame(data={\"PassengerId\":test[\"PassengerId\"], \"Survived\":result})\n# output.to_csv(\"STACK_Ensemble.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"497864667097ec79e44bcfa3fbfb119e40f30897"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}