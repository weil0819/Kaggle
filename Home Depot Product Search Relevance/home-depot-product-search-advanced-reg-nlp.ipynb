{"cells":[{"metadata":{"_uuid":"738d4e587c17ccb7c8f85438923a15307b6457b9"},"cell_type":"markdown","source":"# Abstract\n### Problem\npredict the relevance for each pair listed in the test set\n\n### Method\nFor NLP problem, we often need to generate some self-made text features to help ML model.  \nHow to generate slef-made text features?  \n\n- String Distance\n- TF-IDF\n- Word2Vec"},{"metadata":{"_uuid":"c8b8aba8a13a2067cf6a38fc080cbf26de036d23"},"cell_type":"markdown","source":"# 0. Import Necessary Packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor\nfrom nltk.stem.snowball import SnowballStemmer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# 1. Read Dataset"},{"metadata":{"trusted":true,"_uuid":"cd519917720ca510d6d1e3648674cecde2b15892"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")\ntest_df = pd.read_csv('../input/test.csv', encoding=\"ISO-8859-1\")\ndesc_df = pd.read_csv('../input/product_descriptions.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16151ba7498e33dedba9441b1626307cdd64d9f5"},"cell_type":"markdown","source":"# 2. Dataset Summary"},{"metadata":{"trusted":true,"_uuid":"23af3c0a177fd52bd29956742a74e8c76b8c8f20"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e63c58137d9657be150d31339045c4610b4dab28"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"346890943a03a4e89395f2eb434e57db34540611"},"cell_type":"code","source":"desc_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc2cbde2e7df2efb7c718de335fb7f5686afbbe0"},"cell_type":"markdown","source":"# 3. Combine Train and Test Dataset"},{"metadata":{"trusted":true,"_uuid":"414aeef0abd34987950f38c94ea92433ad467e36"},"cell_type":"code","source":"all_df = pd.concat((train_df, test_df), axis=0, ignore_index=True)\nall_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0704ed63928af04c122d2c7e6199585c7734e375"},"cell_type":"code","source":"print(\"Number of instance: \", all_df.shape[0])\nprint(\"Number of features: \", all_df.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f95abbb24f8ad6c88f35dd8e224ea54c1333d63"},"cell_type":"code","source":"# Merge product description.\nall_df = pd.merge(all_df, desc_df, how='left', on='product_uid')\nall_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"972412725fb26aa3e42f031eed52b44104d49a54"},"cell_type":"markdown","source":"# 4. Text Preprocessing\nFor text preprocessing, we have various operations, such like stopwords, drop_number, lemma, stem...   \nIn this case, we just use stem"},{"metadata":{"trusted":true,"_uuid":"ee72e731a04f66fe8be7cee6368adf6ee7393610"},"cell_type":"code","source":"stemmer = SnowballStemmer('english')\n\ndef str_stemmer(s):\n    return \" \".join([stemmer.stem(word) for word in s.lower().split()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"773d7aa13ee18abe684bcd8762d22862c341999b"},"cell_type":"code","source":"all_df['search_term'] = all_df['search_term'].map(lambda x:str_stemmer(x))\nall_df['product_title'] = all_df['product_title'].map(lambda x:str_stemmer(x))\nall_df['product_description'] = all_df['product_description'].map(lambda x:str_stemmer(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcf8a387c1f8f221ebd5e9072d25f27de6ee075d"},"cell_type":"markdown","source":"# 5. Self-made Text Features (optional)"},{"metadata":{"_uuid":"37ac8af34531a96a9a9adcef2f606d7072dfe2ef"},"cell_type":"markdown","source":"### Levenshtein\n- ratio(string1, string2)\n  - Compute similarity of two strings.\n  - https://rawgit.com/ztane/python-Levenshtein/master/docs/Levenshtein.html#Levenshtein-ratio\n  - Use this function, we can add two new features -- 'dist_in_title' and 'dist_in_desc'"},{"metadata":{"trusted":true,"_uuid":"ec711da7a2d2433d688bc48373d6891ba852b39b"},"cell_type":"code","source":"import Levenshtein","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04e2c1af39e4a4ab9bd526bb89e8392779d6d186"},"cell_type":"code","source":"Levenshtein.ratio('hello', 'hello world')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77ec3217d1f1214cdd576244c86f7ea22f68dcbc"},"cell_type":"code","source":"all_df['dist_in_title'] = all_df.apply(lambda x:Levenshtein.ratio(\n    x['search_term'],x['product_title']), axis=1)\nall_df['dist_in_desc'] = all_df.apply(lambda x:Levenshtein.ratio(\n    x['search_term'],x['product_description']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"455a3adab6921e46af24589d48f24024779d994e"},"cell_type":"markdown","source":"### TF-iDF\n- Step 1: create a new column, including all free-text\n- Step 2: build a term dictionary by using gensim/sklearn\n- Step 3: Bag-of-Words\n  - The function **doc2bow()** simply counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. \n- Step 4: create our own corpus\n- Step 5: build TF-IDF model\n- Step 6: compare similarity between two terms"},{"metadata":{"trusted":true,"_uuid":"3db5d98994dcb605fe282827d41ea8ddde656db1"},"cell_type":"code","source":"# Merge all free-text features as one new feature.\nall_df['all_texts'] = all_df['product_title'] + ' . ' + all_df['product_description'] \n+ ' . '","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da2e3da9db86548ca7e57fdff3458814bee2c2f7"},"cell_type":"code","source":"all_df['all_texts'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f524d09c1166c1f5d0143d5ac5885be6e62b0b4a"},"cell_type":"code","source":"from gensim.utils import tokenize\nfrom gensim.corpora.dictionary import Dictionary\ndictionary = Dictionary(list(tokenize(x, errors='ignore')) \n                        for x in all_df['all_texts'].values) # fit dictionary\nprint(dictionary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4b3d90ee08191e966ea3c2fe1ea374a86143ec1"},"cell_type":"code","source":"# Bag-of-word: To convert documents to vectors, convert corpus to BoW format.\nclass MyCorpus(object):\n    def __iter__(self):\n        for x in all_df['all_texts'].values:\n            # convert corpus to BoW format\n            yield dictionary.doc2bow(list(tokenize(x, errors='ignore')))\n\ncorpus = MyCorpus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2ed90e56ee2fd1f3603c3dc93a607f05acd2928"},"cell_type":"code","source":"# Build TF-IDF model.\nfrom gensim.models.tfidfmodel import TfidfModel\ntfidf = TfidfModel(corpus) # fit model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"85f3cf598f8032c7abe49726ea88729cfdbc5a8b"},"cell_type":"code","source":"# Test TF-IDF model.\ntfidf[dictionary.doc2bow(list(tokenize('hello world, good morning', errors='ignore')))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"151153dfe27fe1d1f1e6971e112b108e310aace8"},"cell_type":"code","source":"from gensim.similarities import MatrixSimilarity\n\ndef to_tfidf(text):\n    res = tfidf[dictionary.doc2bow(list(tokenize(text, errors='ignore')))]\n    return res\n\ndef cos_sim(text1, text2):\n    tfidf1 = to_tfidf(text1)\n    tfidf2 = to_tfidf(text2)\n    index = MatrixSimilarity([tfidf1],num_features=len(dictionary))\n    sim = index[tfidf2]\n    return float(sim[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"929c58e706d2ace9a0929363d0e9bd8b72e491c7"},"cell_type":"code","source":"# Test cosine similarity.\ntext1 = 'hello world'\ntext2 = 'hello from the other side'\ncos_sim(text1, text2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"337239ca243d0a3c471793819990126eb1094a3c"},"cell_type":"code","source":"# Generate two new features -- 'tfidf_cos_sim_in_title' and 'tfidf_cos_sim_in_desc'.\nall_df['tfidf_cos_sim_in_title'] = all_df.apply(lambda x: cos_sim(\n    x['search_term'], x['product_title']), axis=1)\nall_df['tfidf_cos_sim_in_desc'] = all_df.apply(lambda x: cos_sim(\n    x['search_term'], x['product_description']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"824b302d88195ebc39a35c5fcc07217ea1dd81a0"},"cell_type":"code","source":"# Check new feature value.\nall_df['tfidf_cos_sim_in_title'][:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"599c8c764497152bfc7e2c0c528324e9f76f844b"},"cell_type":"markdown","source":"### Word2Vec\n- Step 1: split text into list of sentences\n- Step 2: split sentences into list of words in order to build corpur\n  - nltk.tokenize.word_tokenize\n    - https://www.nltk.org/api/nltk.tokenize.html\n  - gensim.utils.tokenize\n    - https://radimrehurek.com/gensim/utils.html#gensim.utils.tokenize\n- Step 3: train Word2Vec model\n  - https://radimrehurek.com/gensim/models/word2vec.html\n- Step 4: compare similarity between two terms"},{"metadata":{"trusted":true,"_uuid":"fcb7f07972d9820241a5b5dbc2d09e4957976e22"},"cell_type":"code","source":"import nltk\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33006efae8142aa084316e86bee3b2b4f1d1948c"},"cell_type":"code","source":"# Test sentences splitting.\ntokenizer.tokenize(all_df['all_texts'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bd36ecdf3e9ea9c637b9e8f7467b5ba24fece02"},"cell_type":"code","source":"# free-text ==> sentences\nsentences = [tokenizer.tokenize(x) for x in all_df['all_texts'].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a369e2dd982576ddbcbfde1691b4872037c4b071"},"cell_type":"code","source":"# Flatten list of lists due to there is no hierarchical relationship in sentences.\nsentences = [y for x in sentences for y in x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dac24d8f3da50c22ac6cc32bdc3e78c682c1dc68"},"cell_type":"code","source":"len(sentences) # how many sentences in total ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08ff68ae64ea8004bb32ee3efbe016340602b86e"},"cell_type":"code","source":"# Build corpur, Sentences ==> words\nfrom nltk.tokenize import word_tokenize\nw2v_corpus = [word_tokenize(x) for x in sentences]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93041a8a44e74773749b225980929042524b95b5"},"cell_type":"code","source":"# Train the model.\nfrom gensim.models.word2vec import Word2Vec\n\nmodel = Word2Vec(w2v_corpus, size=128, window=5, min_count=5, workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d13fc5780b43f489c1a87701c15eacd92c3e5f66"},"cell_type":"code","source":"# Test term -- 'right', returns a vector.\nmodel['right']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c476b80e410057117453c16432aea43a38701b0"},"cell_type":"code","source":"# Get all vocabulary.\nvocab = model.wv\n\n# Get corresponding vector of any text.\ndef get_vector(text):\n    res =np.zeros([128])\n    count = 0\n    for word in word_tokenize(text):\n        if word in vocab:\n            res += model[word]\n            count += 1\n    return res/count  # compute the average w2v vector of each word in text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c06963b2d1fad77c41e2e740164d4da2a2a3e41"},"cell_type":"code","source":"# Test get_vector.\nprint(get_vector('life is like a box of chocolate'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2b91ffbe5c49df4a54eb296c568a55ee014cd6e"},"cell_type":"code","source":"from scipy import spatial\n\ndef w2v_cos_sim(text1, text2):\n    try:\n        w2v1 = get_vector(text1)\n        w2v2 = get_vector(text2)\n        sim = 1 - spatial.distance.cosine(w2v1, w2v2)\n        return float(sim)\n    except:\n        return float(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c34c2944ed80e9789c7d05de4fb82486c36d343"},"cell_type":"code","source":"# Test w2v_cos_sim.\nw2v_cos_sim('hello world', 'hello from the other side')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0b82305676b34762c10c5082c4b7aeabecfb71b"},"cell_type":"code","source":"# Generate two new features -- 'w2v_cos_sim_in_title' and 'w2v_cos_sim_in_desc'.\nall_df['w2v_cos_sim_in_title'] = all_df.apply(\n    lambda x: w2v_cos_sim(x['search_term'], x['product_title']), axis=1)\nall_df['w2v_cos_sim_in_desc'] = all_df.apply(\n    lambda x: w2v_cos_sim(x['search_term'], x['product_description']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65974c42b1f47adf39e8387464962fd08dccf537"},"cell_type":"code","source":"# Show current dataframe.\nall_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60c53caa310a3b2ded54639bafb54e590b9bed78"},"cell_type":"code","source":"# Drop all features that can not be inputted into ML model.\nall_df = all_df.drop(\n    ['search_term','product_title','product_description','all_texts'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1566ec1eac40ab3b9043e67be8cc7af534685b51"},"cell_type":"code","source":"# Show current dataframe,all features are numerical data type.\nall_df.head(5) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8650c89ab969a9b3d82f9bc0b1bf9576d6fee14"},"cell_type":"code","source":"# Fill all NaN with 0.\nall_df = all_df.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"198f66b53a8367940b647f099ef05ff2f5aa64bf"},"cell_type":"markdown","source":"# 6. Reshape Train and Test Data"},{"metadata":{"trusted":true,"_uuid":"bd27a4461570cbed08cc45891742cb6b6e4b8f1f"},"cell_type":"code","source":"# Seperate train and test data.\ntrain_df = all_df.loc[train_df.index]\ntest_df = all_df.loc[test_df.index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd9069225fea147fd02c621c471a098ef88fb11b"},"cell_type":"code","source":"# keep test index.\ntest_ids = test_df['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d40979128567eb22a92ad8900d69c06879d4c1e"},"cell_type":"code","source":"# Extract target values of train data.\ny_train = train_df['relevance'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a0ea4abbafe5af633e70ab132528e61a9ae2268"},"cell_type":"code","source":"# For train and test data, drop all label features.\nX_train = train_df.drop(['id','relevance'],axis=1).values\nX_test = test_df.drop(['id','relevance'],axis=1).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79e3355f67e3e7c5012983dc98fa8f49219ccd83"},"cell_type":"markdown","source":"# 7. Modeling"},{"metadata":{"trusted":true,"_uuid":"95afc7fffac582cfa8f8434b9898f20838abd1a1"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"898ccbf514473ab88546c5d684539b8ff47f132c"},"cell_type":"code","source":"params = [1,3,5,6,7,8,9,10]\ntest_scores = []\nfor param in params:\n    clf = RandomForestRegressor(n_estimators=30, max_depth=param)\n    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=5, \n                                          scoring='neg_mean_squared_error'))\n    test_scores.append(np.mean(test_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35cda4685e2e300c95dac5d73e8482862436d11a"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(params, test_scores)\nplt.title(\"Param vs CV Error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b83cc4126356c1ddeb45147a3624aefce762c336"},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=30, max_depth=6)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16c3db1bc81dbb3572daaafe24646a7a08f2871a"},"cell_type":"markdown","source":"# 8. Prediction"},{"metadata":{"trusted":true,"_uuid":"6ca86ca86652f2001993fe88be6128eb591f0584"},"cell_type":"code","source":"y_pred = rf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1587b2dc19fb6ae56fe9390997b69b652a8bce01"},"cell_type":"markdown","source":"# 9. Submission"},{"metadata":{"trusted":true,"_uuid":"858317271a699f335c79fe48c5a5d6452f5760d3"},"cell_type":"code","source":"pd.DataFrame({\"id\": test_ids, \"relevance\": y_pred}).to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a8b0dcb192c854337e86f04a0c98a314a63a27c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}